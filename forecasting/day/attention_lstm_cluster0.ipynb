{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with random seed 743\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:73:00.0, compute capability: 8.6\n",
      "/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: NVIDIA RTX A6000, pci bus id: 0000:a6:00.0, compute capability: 8.6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 16:49:12.603694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46713 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-12-14 16:49:12.604297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46181 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:73:00.0, compute capability: 8.6\n",
      "2022-12-14 16:49:12.604849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 46713 MB memory:  -> device: 2, name: NVIDIA RTX A6000, pci bus id: 0000:a6:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `config` must be a tf.ConfigProto, but got \"Session\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m session_conf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession(config\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mConfigProto(log_device_placement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m sess \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m K\u001b[38;5;241m.\u001b[39mset_session(sess)\n",
      "File \u001b[0;32m/programdrive/anaconda3/envs/clust/lib/python3.8/site-packages/tensorflow/python/client/session.py:1603\u001b[0m, in \u001b[0;36mSession.__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1583\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a new TensorFlow session.\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \n\u001b[1;32m   1585\u001b[0m \u001b[38;5;124;03m  If no `graph` argument is specified when constructing the session,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;124;03m        protocol buffer with configuration options for the session.\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1603\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m   \u001b[38;5;66;03m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_graph_context_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/programdrive/anaconda3/envs/clust/lib/python3.8/site-packages/tensorflow/python/client/session.py:689\u001b[0m, in \u001b[0;36mBaseSession.__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    686\u001b[0m   config \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcontext()\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, config_pb2\u001b[38;5;241m.\u001b[39mConfigProto):\n\u001b[0;32m--> 689\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `config` must be a tf.ConfigProto, but got \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    690\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(config)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (mixed_precision_global_state\u001b[38;5;241m.\u001b[39mis_mixed_precision_graph_rewrite_enabled()\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mgraph_options\u001b[38;5;241m.\u001b[39mrewrite_options\u001b[38;5;241m.\u001b[39mauto_mixed_precision \u001b[38;5;241m!=\u001b[39m\n\u001b[1;32m    694\u001b[0m     rewriter_config_pb2\u001b[38;5;241m.\u001b[39mRewriterConfig\u001b[38;5;241m.\u001b[39mOFF):\n\u001b[1;32m    695\u001b[0m   new_config \u001b[38;5;241m=\u001b[39m config_pb2\u001b[38;5;241m.\u001b[39mConfigProto()\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `config` must be a tf.ConfigProto, but got \"Session\""
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# 모델의 reproducibility를 위해 random seed를 고정함\n",
    "seed_value = 743\n",
    "print(\"Train with random seed\", seed_value)\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 그래프를 그리기 위한 matplotlib 및 기타 utility 라이브러리를 import함\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as font_manager\n",
    "# matplotlib를 사용해 그래프를 그릴 때 사용할 글꼴을 설정함\n",
    "font_dirs = ['.']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = font_manager.createFontList(font_files)\n",
    "font_manager.fontManager.ttflist.extend(font_list)\n",
    "matplotlib.rcParams['font.family'] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler  # 데이터 정규화에 사용할 MinMaxScaler import\n",
    "from sklearn.metrics import mean_squared_error  # MSE 성능 지표를 계산하기 하기 위한 함수 import\n",
    "\n",
    "# 모델을 구축하기 위한 keras 관련 함수 import\n",
    "from keras.models import *\n",
    "from keras.layers import Lambda, RepeatVector\n",
    "from keras.layers import Input, multiply\n",
    "from keras.layers import Dense, LSTM, Dropout, Flatten\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"MAPE 성능 지표를 계산하기 위한 함수 정의\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def calculate_smape(actual, predicted) -> float:\n",
    "    \"\"\"SMAPE 성능 지표를 계산하기 위한 함수 정의\"\"\"\n",
    "\n",
    "    # Convert actual and predicted to numpy\n",
    "    # array data type if not already\n",
    "    if not all([isinstance(actual, np.ndarray), \n",
    "                isinstance(predicted, np.ndarray)]):\n",
    "        actual, predicted = np.array(actual),\n",
    "        np.array(predicted)\n",
    "  \n",
    "    return round(\n",
    "        np.mean(\n",
    "            np.abs(predicted - actual) / \n",
    "            ((np.abs(predicted) + np.abs(actual))/2)\n",
    "        )*100, 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터링을 진행한 CSV 데이터 파일의 path를 설정함\n",
    "data_filename = '../data/PM10_clustering_1day_result_1014.csv'\n",
    "detrended_df = pd.read_csv(data_filename)  # Pandas의 read_csv 함수를 이용해 CSV 파일을 읽음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_df['Cluster'].value_counts()  # 각 클러스터에 포함된 시계열 데이터의 샘플 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_df = detrended_df[detrended_df['Cluster'] == 'Cluster 0']  # 특정 클러스터의 데이터만 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_row_num = len(specific_df)  # 선택한 클러스터의 시계열 데이터의 샘플 수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_row_index(total_row, train_split=0.6):\n",
    "    \"\"\"DataFrame을 train, test, validation으로 나누기 위한 helper 함수\"\"\"\n",
    "    train_data_up = int(total_row * train_split)\n",
    "    remain_data_row = cluster_row_num - train_data_up\n",
    "    \n",
    "    # test set과 validation set의 row 수가 같도록\n",
    "    # train, test, valid split을 진행함\n",
    "    if remain_data_row % 2 == 1:\n",
    "        train_data_up += 1\n",
    "        remain_data_row -= 1\n",
    "    \n",
    "    valid_data_up = int(remain_data_row / 2)\n",
    "    \n",
    "    assert train_data_up + valid_data_up * 2 == total_row\n",
    "    \n",
    "    return train_data_up, train_data_up + valid_data_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up_bound, valid_up_bound = get_split_row_index(cluster_row_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 DataFrame을 train, test, validation DataFrame으로 나눔\n",
    "train_df = specific_df[:train_up_bound]\n",
    "valid_df = specific_df[train_up_bound:valid_up_bound]\n",
    "test_df = specific_df[valid_up_bound:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, validation DataFrame의 시계열 데이터 샘플 수 확인\n",
    "print(\"Train DataFrame row: \", len(train_df))\n",
    "print(\"Valid DataFrame row: \", len(valid_df))\n",
    "print(\"Test DataFrame row: \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_df) + len(valid_df) + len(test_df) == cluster_row_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(valid_df.index.values)  # validation에 사용한 시계열 데이터 샘플의 index id 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_df.index.values)  # test에 사용한 시계열 데이터 샘플의 index id 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame에서 `Cluster` column을 삭제함\n",
    "train_df = train_df.drop(['Cluster'], axis=1).reset_index(drop=True)\n",
    "valid_df = valid_df.drop(['Cluster'], axis=1).reset_index(drop=True)\n",
    "test_df = test_df.drop(['Cluster'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 DataFrame에 포함된 0 값의 갯 수를 확인함\n",
    "print(\"Train Data - Number of 0: \", (train_df == 0).sum().sum())\n",
    "print(\"Valid Data - Number of 0: \", (valid_df == 0).sum().sum())\n",
    "print(\"Test Data - Number of 0: \", (test_df == 0).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_list(data_df):\n",
    "    \"\"\"DataFrame을 list로 변환하기 위한 helper 함수 정의\"\"\"\n",
    "    total_data = []  # 전체 데이터 샘플 리스트\n",
    "    data_list = []  # 데이터 샘플의 리스트\n",
    "    for i in range(len(data_df)):\n",
    "        i_row = data_df.loc[i, :].tolist()\n",
    "        total_data.extend(i_row)\n",
    "        data_list.append(i_row)\n",
    "    \n",
    "    return total_data, data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 list 형태로 변환함\n",
    "train_data_total, train_data_clusters = dataframe_to_list(train_df)\n",
    "valid_data_total, valid_data_clusters = dataframe_to_list(valid_df)\n",
    "test_data_total, test_data_clusters = dataframe_to_list(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_clusters(scaler, data_clusters):\n",
    "    \"\"\"Scaler를 사용해 각 시계열 데이터 샘플에 대해 정규화를 진행하는 함수를 정의\"\"\"\n",
    "    std_data_clusters = []\n",
    "    for data_cluster in data_clusters:\n",
    "        # scaler의 transform 함수를 사용해 정규화를 진행함\n",
    "        std_data_cluster = scaler.transform(np.array(data_cluster).reshape(-1, 1))\n",
    "        std_data_cluster = std_data_cluster.reshape(-1).tolist()\n",
    "        std_data_clusters.append(std_data_cluster)\n",
    "    return std_data_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()  # MinMaxScaler 정규화 객체 생성\n",
    "scaler.fit(np.array(train_data_total).reshape(-1, 1))\n",
    "# 각 데이터 클러스터에 대해 정규화를 진행함\n",
    "std_train_data_clusters = scale_data_clusters(scaler, train_data_clusters)\n",
    "std_valid_data_clusters = scale_data_clusters(scaler, valid_data_clusters)\n",
    "std_test_data_clusters = scale_data_clusters(scaler, test_data_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(std_train_data_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_cluster(data_cluster, window_size=6, look_ahead=6):\n",
    "    \"\"\"List 형식의 data_cluster를 사용해 모델 입력을 위한\n",
    "    (batch_size, window_size, 1) 형식으로 변환함\"\"\"\n",
    "    data_x, data_y = [], []\n",
    "    assert isinstance(data_cluster, list)\n",
    "    data_cluster = np.array(data_cluster)\n",
    "    for i in range(len(data_cluster) - window_size - look_ahead):\n",
    "        x = data_cluster[i: (i + window_size)]\n",
    "        y = data_cluster[i + window_size + look_ahead]\n",
    "        data_x.append(x)  # window_size 만큼의 데이터를 입력으로 사용\n",
    "        data_y.append(y)  # look_ahead 이후의 데이터를 예측함\n",
    "    return np.array(data_x).reshape(-1, window_size, 1), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cluster = std_train_data_clusters[0][:15]\n",
    "print(sample_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_cluster(sample_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_clusters):\n",
    "    \"\"\"wrap_cluster 함수를 이용해 dataset 생성\"\"\"\n",
    "    X, y = [], []\n",
    "    for data_cluster in data_clusters:\n",
    "        cluster_X, cluster_y = wrap_cluster(data_cluster)\n",
    "        X.append(cluster_X)\n",
    "        y.append(cluster_y)\n",
    "        \n",
    "    # np.concatenate 함수를 이용해 여러 게의 numpy array를 하나로 concat 함\n",
    "    # [(1, window_size, 1), (1, window_size, 1)] -> (2, window_size, 1)\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, validation 데이터 각각에 대해\n",
    "# create_dataset 함수를 사용해 데이터 셋을 생성함\n",
    "train_X, train_y = create_dataset(std_train_data_clusters)\n",
    "valid_X, valid_y = create_dataset(std_valid_data_clusters)\n",
    "test_X, test_y = create_dataset(std_test_data_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape, train_y.shape)\n",
    "print(valid_X.shape, valid_y.shape)\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs, input_dim, single_attention_vector):\n",
    "    \"\"\"Feature attention block 정의\"\"\"\n",
    "    time_steps = int(inputs.shape[1])\n",
    "    # Attention weights 계산\n",
    "    a = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)  # (batch_size, input_dim, time_step)\n",
    "    if single_attention_vector:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)  # (batch_size, input_dim)\n",
    "        a = RepeatVector(time_steps)(a)  # (batch_size, input_dim, time_step)\n",
    "    output_attention_mul = multiply([inputs, a], name='attention_mul')  # Attention weights 적용\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_before_lstm(batch_size, time_step, feature_num, single_attention_vector):\n",
    "    \"\"\"Attention LSTM 모델 정의\"\"\"\n",
    "    inputs = Input(shape=(time_step, feature_num))\n",
    "    x = attention_3d_block(inputs, feature_num, single_attention_vector)\n",
    "    x = LSTM(6, activation='tanh',\n",
    "             stateful=False,\n",
    "             return_sequences=True,\n",
    "             kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10, activation='linear', kernel_regularizer=regularizers.l2(0.01),\n",
    "              activity_regularizer=regularizers.l1(0.))(x)\n",
    "    output = Dense(1, activation='linear', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 정의\n",
    "batch_size = 4\n",
    "look_back = 6\n",
    "feature_num = 1\n",
    "SINGLE_ATTENTION_VECTOR = True\n",
    "\n",
    "# LSTM 모델을 생성하고 compile를 진행함\n",
    "model = model_attention_applied_before_lstm(batch_size, look_back, feature_num, SINGLE_ATTENTION_VECTOR)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 진행\n",
    "# 훈련 과정의 손실값을 history 변수에 저장\n",
    "history = model.fit(train_X, train_y,\n",
    "                    validation_data=(valid_X, valid_y),\n",
    "                    batch_size=batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loss와 validation loss의 변화를 matplotlib를 사용해 시각화함\n",
    "plt.plot(loss, label='loss')\n",
    "plt.plot(val_loss, label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련한 모델을 사용해 예측을 진행함\n",
    "train_predict = model.predict(train_X, batch_size)\n",
    "valid_predict = model.predict(valid_X, batch_size)\n",
    "test_predict = model.predict(test_X, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 측정을 위해 예측한 값에 대해 inverse transform을 진행함\n",
    "inv_train_y = scaler.inverse_transform(train_y.reshape(-1, 1))\n",
    "inv_train_predict = scaler.inverse_transform(train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_valid_y = scaler.inverse_transform(valid_y.reshape(-1, 1))\n",
    "inv_valid_predict = scaler.inverse_transform(valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_test_y = scaler.inverse_transform(test_y.reshape(-1, 1))\n",
    "inv_test_predict = scaler.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, validation set에 대해 MAPE 계산\n",
    "train_mape = mean_absolute_percentage_error(inv_train_y, inv_train_predict)\n",
    "valid_mape = mean_absolute_percentage_error(inv_valid_y, inv_valid_predict)\n",
    "test_mape = mean_absolute_percentage_error(inv_test_y, inv_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train MAPE: %.2f\" % train_mape)\n",
    "print(\"Valid MAPE: %.2f\" % valid_mape)\n",
    "print(\"Test MAPE: %.2f\" % test_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, validation set에 대해 SMAPE 계산\n",
    "train_smape = calculate_smape(inv_train_y, inv_train_predict)\n",
    "valid_smape = calculate_smape(inv_valid_y, inv_valid_predict)\n",
    "test_smape = calculate_smape(inv_test_y, inv_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train SMAPE: %.2f\" % train_smape)\n",
    "print(\"Valid SMAPE: %.2f\" % valid_smape)\n",
    "print(\"Test SMAPE: %.2f\" % test_smape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, validation set에 대해 MSE 계산\n",
    "train_mse = mean_squared_error(inv_train_y, inv_train_predict)\n",
    "valid_mse = mean_squared_error(inv_valid_y, inv_valid_predict)\n",
    "test_mse = mean_squared_error(inv_test_y, inv_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train MSE: %.2f\" % train_mse)\n",
    "print(\"Valid MSE: %.2f\" % valid_mse)\n",
    "print(\"Test MSE: %.2f\" % test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# matplotlib을 이용해 train, test, validation set에 대한 실제값과 예측값을 시각화함\n",
    "train_term = len(inv_train_y)\n",
    "valid_term = len(inv_train_y) + len(inv_valid_y)\n",
    "total_sample = len(inv_train_y) + len(inv_valid_y) + len(inv_test_y)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(np.arange(train_term), inv_train_y, color='red', ls='-', label='Real Train Data')\n",
    "plt.plot(np.arange(train_term), inv_train_predict, color='blue', ls='--', label='Predict Train Data')\n",
    "plt.plot(np.arange(train_term, valid_term), inv_valid_y, color='red', ls='-', label='Real Valid Data')\n",
    "plt.plot(np.arange(train_term, valid_term), inv_valid_predict, color='green', ls='--', label='Predict Valid Data')\n",
    "plt.plot(np.arange(valid_term, total_sample), inv_test_y, color='red', ls='-', label='Real Test Data')\n",
    "plt.plot(np.arange(valid_term, total_sample), inv_test_predict, color='black', ls='--', label='Predict Test Data')\n",
    "plt.title('Prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clust",
   "language": "python",
   "name": "clust"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
